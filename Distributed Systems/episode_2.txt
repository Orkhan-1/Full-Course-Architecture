
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          EPISODE 2 â€” FAULT TOLERANCE IN DISTRIBUTED SYSTEMS            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DISTRIBUTED SYSTEM REALITY                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚     Client         Client         Client                    â”‚
â”‚        â”‚              â”‚              â”‚                      â”‚
â”‚        â–¼              â–¼              â–¼                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚  Load    â”‚   â”‚  Load    â”‚   â”‚  Load    â”‚                â”‚
â”‚   â”‚ Balancer â”‚   â”‚ Balancer â”‚   â”‚ Balancer â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚        â”‚              â”‚              â”‚                      â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                       â–¼                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚              â”‚ Service Mesh /   â”‚                           â”‚
â”‚              â”‚ Message Router   â”‚                           â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                        â”‚                                    â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚        â–¼               â–¼               â–¼                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  Node   â”‚â—„â”€â”€â”€â–ºâ”‚  Node   â”‚â—„â”€â”€â”€â–ºâ”‚  Node   â”‚               â”‚
â”‚   â”‚   A     â”‚     â”‚   B     â”‚     â”‚   C     â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚        â”‚               â”‚               â”‚                    â”‚
â”‚        â–¼               â–¼               â–¼                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚Storage  â”‚     â”‚Storage  â”‚     â”‚Storage  â”‚               â”‚
â”‚   â”‚Replica 1â”‚     â”‚Replica 2â”‚     â”‚Replica 3â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


1ï¸âƒ£  TYPES OF FAILURES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    +---------------------+       +----------------------+
    | Crash Failure       |       | Byzantine Failure    |
    | (Fail-Stop)         |       | (Arbitrary behavior) |
    +---------------------+       +----------------------+
            |                             |
            â–¼                             â–¼
     Node stops responding         Node sends inconsistent data
     â†’ easy to detect              â†’ may lie to others
                                    â†’ hard to detect

ğŸŸ¢ **Fail-Stop Model**
   - Node fails silently â€” no more heartbeats.
   - Other nodes quickly detect absence.
   - Easy to replace or restart node.
   - Used in most modern systems like Kafka brokers, ZooKeeper, Kubernetes pods.

ğŸ”´ **Byzantine Model**
   - Node behaves erratically â€” sends conflicting data.
   - Harder to detect, requires voting or consensus.
   - Seen in blockchain, avionics, and safety-critical systems.
   - Example: a sensor reporting different values to different controllers.

2ï¸âƒ£  FAULT TOLERANCE MECHANISMS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£ **Replication**

    +---------+ â†’ +---------+ â†’ +---------+
    | Node 1  |   | Node 2  |   | Node 3  |
    +---------+   +---------+   +---------+

   - Multiple replicas of data or service.
   - Survive node or disk failure.
   - Achieved using **leader-follower** (Kafka, MongoDB) or **multi-leader** (Cassandra).
   - Core tradeoff: **Consistency vs Availability (CAP Theorem)**.

   âš™ï¸ *Example:*
   In Kafka, each partition has one leader and several followers.
   If leader dies, ISR (in-sync replica) list picks a new leader automatically.


2ï¸âƒ£ **Retries & Heartbeats**

    Node A â”€â”€heartbeatâ”€â”€â–¶ Node B
    Node B stops â†’ Node A marks it â€œFAILEDâ€

   - Heartbeats: periodic pings to check aliveness.
   - Retries: resending failed requests (with exponential backoff).
   - Prevents temporary glitches from becoming hard failures.

   âš™ï¸ *Example:*
   Kubernetes uses health probes (`livenessProbe`, `readinessProbe`)
   to restart containers that stop responding.


3ï¸âƒ£ **Checkpointing & Recovery**

    Event â†’ Save State â†’ Crash â†’ Reload State

   - Save intermediate state to disk or object store.
   - On crash, reload from last checkpoint.
   - Reduces recovery time and avoids recomputation.
   - Common in stream processors like Apache Flink and Spark Streaming.

   âš™ï¸ *Analogy:*
   Like saving a video game â€” if you crash, you restart from last save point.


4ï¸âƒ£ **Write-Ahead Logging (WAL)**

    +-------------------+
    | Write-Ahead Log   |
    +-------------------+
    | Log operation     |
    | Apply change      |
    | Save checkpoint   |
    +-------------------+

   âœ… Guarantees durability and atomicity.
   âœ… Even if crash occurs mid-update, data can be recovered.

   **Process:**
   1. Write the intent to the log (append-only file).
   2. Apply change in memory.
   3. Periodically save checkpoints.
   4. On restart â†’ replay all operations after last checkpoint.

   âš™ï¸ *Example:*
   PostgreSQL, HDFS NameNode, and Raft all use WAL to recover cleanly.


5ï¸âƒ£ **Replication for Fault Tolerance**

    +---------+     +---------+     +---------+
    | Leader  | --> | Follower| --> | Follower|
    +---------+     +---------+     +---------+

   - Leader handles writes, followers replicate state.
   - On crash â†’ new leader is elected.
   - Followers serve reads (depending on config).

   âš–ï¸ **Tradeoffs**
   - Synchronous replication â†’ strong consistency but slower writes.
   - Asynchronous replication â†’ high performance but eventual consistency.

   âš™ï¸ *Example:*
   In MongoDB, the Primary replicates to Secondaries.
   If the Primary fails, a new Primary is auto-elected by a majority vote.

7ï¸âƒ£  REAL-WORLD CASE STUDIES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âš™ï¸ **Kafka:**
      - Uses leader-follower replication.
      - Ensures durability via write-ahead log (WAL).
      - Detects broker crashes using ZooKeeper or KRaft.

   âš™ï¸ **HDFS (Hadoop Distributed File System):**
      - NameNode checkpoint + journal (WAL).
      - DataNode replication (usually 3 copies).
      - Failover to standby NameNode.

   âš™ï¸ **Kubernetes:**
      - Control plane runs replicated API servers.
      - etcd database uses Raft for consensus.
      - Auto-healing: restarts failed containers.

   âš™ï¸ **Cassandra:**
      - Multi-leader replication (AP system).
      - Tunable consistency via quorum reads/writes.
